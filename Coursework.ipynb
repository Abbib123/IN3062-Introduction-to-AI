{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Abbib123/IN3062-Introduction-to-AI/blob/main/Coursework.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# All our code so far, Testing in Google colab"
      ],
      "metadata": {
        "id": "-SOKG3huifG5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_M-QBmAa-3-W",
        "outputId": "b2ef65c2-f46d-41d6-f91d-67f8fbd74db9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t393N-APiHFu",
        "outputId": "8dd04a20-ca2a-4b7a-fe77-c826c6485c5b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A:{'count': 6392, 'variance': 0.047415510588180096, 'mean': 0.04990613266583229, 'sdev': 0.2177510289026899}\n",
            "C:{'count': 6392, 'variance': 0.05893582168574595, 'mean': 0.06289111389236546, 'sdev': 0.24276701111507296}\n",
            "D:{'count': 6392, 'variance': 0.2218209783114584, 'mean': 0.3321339173967459, 'sdev': 0.4709787450739772}\n",
            "G:{'count': 6392, 'variance': 0.05825137237409579, 'mean': 0.06210888610763454, 'sdev': 0.24135321082201452}\n",
            "H:{'count': 6392, 'variance': 0.03074984903688736, 'mean': 0.03175844806007509, 'sdev': 0.1753563487213604}\n",
            "ID:{'count': 6392, 'variance': 2009159.1969048455, 'mean': 2271.150813516896, 'sdev': 1417.4481284706137}\n",
            "Left-Diagnostic Keywords:{'count': 0, 'sum': 0}\n",
            "Left-Fundus:{'count': 0, 'sum': 0}\n",
            "M:{'count': 6392, 'variance': 0.04558057944770937, 'mean': 0.047872340425531915, 'sdev': 0.21349608766370726}\n",
            "N:{'count': 6392, 'variance': 0.22065360858613584, 'mean': 0.32869211514392993, 'sdev': 0.4697378083422026}\n",
            "O:{'count': 6392, 'variance': 0.1867153246940605, 'mean': 0.24843554443053817, 'sdev': 0.4321056869494551}\n",
            "Patient Age:{'count': 6392, 'variance': 137.51830667558633, 'mean': 57.85794743429287, 'sdev': 11.72681997284798}\n",
            "Patient Sex:{'count': 0, 'sum': 0}\n",
            "Right-Diagnostic Keywords:{'count': 0, 'sum': 0}\n",
            "Right-Fundus:{'count': 0, 'sum': 0}\n",
            "filename:{'count': 0, 'sum': 0}\n",
            "filepath:{'count': 0, 'sum': 0}\n",
            "labels:{'count': 0, 'sum': 0}\n",
            "target:{'count': 0, 'sum': 0}\n",
            "Target after one-hot encoding in y_train[0]: [0. 0. 1. 0. 0. 0. 0. 0.]\n",
            "Target after one-hot encoding in y_train[1]: [0. 0. 0. 0. 0. 0. 1. 0.]\n",
            "Target after one-hot encoding in y_train[2]: [0. 0. 0. 0. 0. 0. 1. 0.]\n",
            "Target after one-hot encoding in y_train[3]: [0. 0. 0. 0. 0. 0. 1. 0.]\n",
            "Target after one-hot encoding in y_train[4]: [0. 0. 0. 1. 0. 0. 0. 0.]\n"
          ]
        }
      ],
      "source": [
        "from ssl import PROTOCOL_TLSv1_2\n",
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "import io\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import keras.utils\n",
        "import seaborn as sns\n",
        "import ast\n",
        "import codecs\n",
        "import csv\n",
        "import math\n",
        "from tensorflow import keras\n",
        "from sklearn import metrics\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from PIL import Image\n",
        "from collections import Counter\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.datasets import fetch_lfw_people\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Flatten, Dropout\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras import regularizers\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.applications import MobileNetV2\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "# Sets the path for the directory\n",
        "path = \"/content/drive/MyDrive/Colab-Notebooks/IN3062-Introduction-to-AI\"\n",
        "encoding = 'utf-8'\n",
        "filename_read = os.path.join(path, \"full_df.csv\")\n",
        "df = pd.read_csv(filename_read)\n",
        "# print(df[0:6392])\n",
        "\n",
        "# SUMMARY STATS BEGINS\n",
        "with codecs.open(filename_read,\"r\",encoding) as fh:\n",
        "  reader = csv.reader(fh)\n",
        "  header_idx = {key: value for (value,key) in enumerate(next(reader))}\n",
        "  headers = header_idx.keys()\n",
        "  sum_stats = {key:value for (key,value) in [(key,{'count':0, 'sum':0,'variance':0})for key in headers]}\n",
        "  row_num = 0\n",
        "  for row in reader:\n",
        "    row_num +=1\n",
        "    for name in headers:\n",
        "      try:\n",
        "        value = float(row[header_idx[name]])\n",
        "        field = sum_stats[name]\n",
        "        field['count'] +=1\n",
        "        field['sum'] +=value\n",
        "      except ValueError:\n",
        "        pass\n",
        "  # Mean calc\n",
        "  for field in sum_stats.values():\n",
        "    if (field['count']/row_num) >0.9:\n",
        "      field['mean'] = field['sum']/field['count']\n",
        "      del field['sum']\n",
        "  # Standard Deviation and Variance Calc\n",
        "  fh.seek(0)\n",
        "  for row in reader:\n",
        "    for name in headers:\n",
        "      try:\n",
        "        value = float(row[header_idx[name]])\n",
        "        field = sum_stats[name]\n",
        "        if 'mean' in field:\n",
        "          field['variance'] += (value - field['mean'])**2\n",
        "      except ValueError:\n",
        "        pass\n",
        "  # Standard Deviation Cont.\n",
        "  for field in sum_stats.values():\n",
        "    if 'mean' in field:\n",
        "      field['variance'] /= field['count']\n",
        "      field['sdev'] = math.sqrt(field['variance'])\n",
        "    else:\n",
        "      del field['variance']\n",
        "  # Print our resulting information\n",
        "  for key in sorted(sum_stats.keys()):\n",
        "    print(f\"{key}:{sum_stats[key]}\")\n",
        "# SUMMARY STATS ENDS\n",
        "\n",
        "# CODE FOR DECISION TREE\n",
        "\n",
        "# import os\n",
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "# from PIL import Image\n",
        "# from sklearn.tree import DecisionTreeClassifier\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# from sklearn.metrics import accuracy_score\n",
        "# from sklearn.metrics import confusion_matrix,ConfusionMatrixDisplay\n",
        "\n",
        "# df = df[['labels', 'target', 'filename']]\n",
        "# df['labels'] = df['labels']\n",
        "\n",
        "# print(df['labels'].value_counts())\n",
        "# N_to_delete = 2473\n",
        "# N_del_num = 0\n",
        "# O_to_delete = 408\n",
        "# O_del_num = 0\n",
        "# D_to_delete = 1308\n",
        "# D_del_num = 0\n",
        "# for i in range(0, len(df)):\n",
        "#   if N_del_num == N_to_delete:\n",
        "#     break\n",
        "#   if df['labels'][i] == 'N':\n",
        "#     df = df.drop(i)\n",
        "#     N_del_num += 1\n",
        "\n",
        "# df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "# for i in range(0, len(df)):\n",
        "#   if D_del_num == D_to_delete:\n",
        "#     break\n",
        "#   if df['labels'][i] == 'D':\n",
        "#     df = df.drop(i)\n",
        "#     D_del_num += 1\n",
        "\n",
        "# df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "# for i in range(0, len(df)):\n",
        "#   if O_del_num == O_to_delete:\n",
        "#     break\n",
        "#   if df['labels'][i] == 'O':\n",
        "#     df = df.drop(i)\n",
        "#     O_del_num += 1\n",
        "\n",
        "# df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "# print(df['labels'].value_counts())\n",
        "\n",
        "# df = df[['labels', 'target', 'filename']]\n",
        "# df['labels'] = df['labels']\n",
        "\n",
        "# def load_and_preprocess_images(filenames, target_size=(128, 128)):\n",
        "#     images = []\n",
        "#     for filename in filenames:\n",
        "#         full_path = '/content/drive/MyDrive/Colab-Notebooks/IN3062-Introduction-to-AI/images/' + filename\n",
        "#         img = Image.open(full_path)\n",
        "#         img = img.resize(target_size)\n",
        "#         img_array = np.array(img)\n",
        "#         img_array = img_array / 255.0\n",
        "#         flattened_img = img_array.flatten()\n",
        "#         images.append(flattened_img)\n",
        "#     return np.array(images)\n",
        "\n",
        "# X = df['filename']\n",
        "\n",
        "# X = load_and_preprocess_images(X)\n",
        "\n",
        "# # Convert string representation of lists in 'target' into actual lists\n",
        "# y = np.array([ast.literal_eval(item) for item in df['target']])\n",
        "\n",
        "# # Convert one-hot encoded labels to class labels\n",
        "# y_class_labels = y.argmax(axis=1)\n",
        "\n",
        "# # Split the dataset\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X, y_class_labels, test_size=0.25, random_state=42)\n",
        "\n",
        "# # Decision Tree Classifier\n",
        "# decision_tree = DecisionTreeClassifier(criterion='entropy')\n",
        "# decision_tree.fit(X_train, y_train)\n",
        "\n",
        "# # Predictions and accuracy\n",
        "# y_pred = decision_tree.predict(X_test)\n",
        "# accuracy = accuracy_score(y_test, y_pred)\n",
        "# print('Accuracy: %.2f' % accuracy)\n",
        "\n",
        "# # Confusion matrix\n",
        "# cm = confusion_matrix(y_test, y_pred)\n",
        "# print(cm)\n",
        "\n",
        "# # Define target names for confusion matrix display\n",
        "# target_names = ['N', 'D', 'G', 'C', 'A', 'H', 'M', 'O']\n",
        "\n",
        "# # Confusion Matrix Display\n",
        "# disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=target_names)\n",
        "# disp.plot()\n",
        "\n",
        "# CODE FOR DECISION TREE ENDS\n",
        "\n",
        "# CODE FOR RANDOM FOREST\n",
        "# my code starts here\n",
        "\n",
        "# from sklearn.ensemble import RandomForestClassifier\n",
        "# from sklearn.metrics import accuracy_score, classification_report\n",
        "# from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
        "\n",
        "# SMOTE\n",
        "\n",
        "# df = df[['labels', 'target', 'filename']]\n",
        "# df['labels'] = df['labels']\n",
        "\n",
        "# print(df['labels'].value_counts())\n",
        "# N_to_delete = 2473\n",
        "# N_del_num = 0\n",
        "# O_to_delete = 408\n",
        "# O_del_num = 0\n",
        "# D_to_delete = 1308\n",
        "# D_del_num = 0\n",
        "# for i in range(0, len(df)):\n",
        "#   if N_del_num == N_to_delete:\n",
        "#     break\n",
        "#   if df['labels'][i] == 'N':\n",
        "#     df = df.drop(i)\n",
        "#     N_del_num += 1\n",
        "\n",
        "# df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "# for i in range(0, len(df)):\n",
        "#   if D_del_num == D_to_delete:\n",
        "#     break\n",
        "#   if df['labels'][i] == 'D':\n",
        "#     df = df.drop(i)\n",
        "#     D_del_num += 1\n",
        "\n",
        "# df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "# for i in range(0, len(df)):\n",
        "#   if O_del_num == O_to_delete:\n",
        "#     break\n",
        "#   if df['labels'][i] == 'O':\n",
        "#     df = df.drop(i)\n",
        "#     O_del_num += 1\n",
        "\n",
        "# df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "# print(df['labels'].value_counts())\n",
        "\n",
        "\n",
        "# def load_and_preprocess_images(filenames, target_size=(512, 512)):\n",
        "#     images = []\n",
        "#     for filename in filenames:\n",
        "#         full_path = '/content/drive/MyDrive/Colab-Notebooks/IN3062-Introduction-to-AI/images/' + filename\n",
        "#         img = Image.open(full_path)\n",
        "#         img = img.resize(target_size)\n",
        "#         img_array = np.array(img)\n",
        "#         img_array = img_array / 255.0\n",
        "#         flattened_img = img_array.flatten()\n",
        "#         images.append(flattened_img)\n",
        "#     return np.array(images)\n",
        "\n",
        "# X = df['filename']\n",
        "\n",
        "# X = load_and_preprocess_images(X)\n",
        "\n",
        "# # Creating the target array y\n",
        "# y = np.array(df['target'].tolist())\n",
        "\n",
        "# # Check the shape of X and y\n",
        "# print(\"Shape of X:\", X.shape)\n",
        "# print(\"Shape of y:\", y.shape)\n",
        "\n",
        "\n",
        "# # #Instantiate the model with 10 trees and entropy as splitting criteria\n",
        "# Random_Forest_model = RandomForestClassifier(n_estimators=150,criterion=\"entropy\")\n",
        "\n",
        "# # #Training/testing split\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=7)\n",
        "\n",
        "# # #Train the model\n",
        "# Random_Forest_model.fit(X_train, y_train)\n",
        "\n",
        "# # #make predictions\n",
        "# y_pred = Random_Forest_model.predict(X_test)\n",
        "\n",
        "\n",
        "# # #Calculate accuracy metric\n",
        "# accuracy = accuracy_score(y_pred, y_test)\n",
        "# print('The accuracy is: ',accuracy*100,'%')\n",
        "# print(classification_report(y_test, y_pred))\n",
        "\n",
        "# my code ends here\n",
        "\n",
        "# CODE FOR CNN BELOW:\n",
        "\n",
        "# df = pd.read_csv(filename_read, na_values=['NA', '?'])\n",
        "\n",
        "# headers = list(df.columns.values)\n",
        "# fields = []\n",
        "\n",
        "# for field in fields:\n",
        "#     print(field)\n",
        "\n",
        "# # # Shows before an after drop of columns to show what we had and what we are using for the model\n",
        "# # # print(f\"before drop: {df.columns}\")\n",
        "# df.drop('ID',axis=1, inplace=True)\n",
        "# df.drop('N',axis=1, inplace=True)\n",
        "# df.drop('D',axis=1, inplace=True)\n",
        "# df.drop('G',axis=1, inplace=True)\n",
        "# df.drop('C',axis=1, inplace=True)\n",
        "# df.drop('A',axis=1, inplace=True)\n",
        "# df.drop('H',axis=1, inplace=True)\n",
        "# df.drop('M',axis=1, inplace=True)\n",
        "# df.drop('O',axis=1, inplace=True)\n",
        "# df.drop('Left-Fundus',axis=1, inplace=True)\n",
        "# df.drop('Right-Fundus',axis=1, inplace=True)\n",
        "# df.drop('Left-Diagnostic Keywords',axis=1, inplace=True)\n",
        "# df.drop('Right-Diagnostic Keywords',axis=1, inplace=True)\n",
        "# df.drop('filepath',axis=1, inplace=True)\n",
        "# df.drop('target',axis=1, inplace=True)\n",
        "# df.drop('Patient Age', axis=1,inplace=True)\n",
        "# df.drop('Patient Sex', axis=1,inplace=True)\n",
        "# # print(f\"after drop: {df.columns}\")\n",
        "# # print(df[0:6392])\n",
        "# # print(df.head())\n",
        "\n",
        "# # # Trimming down larger groups to prevent skewing of data towards larger classes - NOT TO BE USED, MAKES DATASET TOO SMALL + INACCURATE\n",
        "# # # print(df['labels'].value_counts())\n",
        "# # N_to_delete = 1073\n",
        "# # N_del_num = 0\n",
        "# # O_to_delete = 408\n",
        "# # O_del_num = 0\n",
        "# # D_to_delete = 1308\n",
        "# # D_del_num = 0\n",
        "# # for i in range(0, len(df)):\n",
        "# #   if N_del_num == N_to_delete:\n",
        "# #     break\n",
        "# #   if df['labels'][i] == 'N':\n",
        "# #     df = df.drop(i)\n",
        "# #     N_del_num += 1\n",
        "\n",
        "# # # df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "# # # for i in range(0, len(df)):\n",
        "# # #   if D_del_num == D_to_delete:\n",
        "# # #     break\n",
        "# # #   if df['labels'][i] == 'D':\n",
        "# # #     df = df.drop(i)\n",
        "# # #     D_del_num += 1\n",
        "\n",
        "# # # df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "# # # for i in range(0, len(df)):\n",
        "# # #   if O_del_num == O_to_delete:\n",
        "# # #     break\n",
        "# # #   if df['labels'][i] == 'O':\n",
        "# # #     df = df.drop(i)\n",
        "# # #     O_del_num += 1\n",
        "\n",
        "# # # df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "# # # print(df['labels'].value_counts())\n",
        "\n",
        "# # End of data trimming\n",
        "\n",
        "# # # Preprocessing images - FOR MODEL2 AND MOBIILENETV2\n",
        "# width = 128\n",
        "# height = 128\n",
        "# def preprocess_image(file_paths, is_training):\n",
        "#     images = []\n",
        "#     data_aug = ImageDataGenerator(\n",
        "#         rescale=1./255,\n",
        "#         rotation_range=20,\n",
        "#         zoom_range=0.2,\n",
        "#         width_shift_range=0.2,\n",
        "#         height_shift_range=0.2,\n",
        "#         shear_range=0.2,\n",
        "#         horizontal_flip=True,\n",
        "#         fill_mode='nearest'\n",
        "#     )\n",
        "#     for file_path in file_paths:\n",
        "#         full_path = '/content/drive/MyDrive/Colab-Notebooks/IN3062-Introduction-to-AI/images/' + file_path\n",
        "#         # Load and preprocess image\n",
        "#         #print(\"Opening\"+full_path)\n",
        "#         img = Image.open(full_path)\n",
        "#         img = img.resize((width, height))\n",
        "#         img_array = np.array(img)\n",
        "#         # Augment data with data_aug\n",
        "#         # Tests if it is part of the training data, if not it will skip the data augmentation\n",
        "#         if is_training==True:\n",
        "#           img_array = data_aug.random_transform(img_array)\n",
        "#           img_array = img_array/255\n",
        "#           images.append(img_array)\n",
        "#         else:\n",
        "#           img_array = img_array/255\n",
        "#           images.append(img_array)\n",
        "#     return np.array(images)\n",
        "\n",
        "# # Prepping test train split for x and y\n",
        "# X = df.filename\n",
        "# y = df.labels\n",
        "# # Split our data into 60/20/20:train/val/test\n",
        "# X_train, X_test_temp, y_train, y_test_temp = train_test_split(X,y,stratify=y,test_size=0.4)\n",
        "# X_val, X_test, y_val, y_test = train_test_split(X_test_temp,y_test_temp,test_size=0.5)\n",
        "# # Preprocess the data - Don't do data augmentation to val/test\n",
        "# # X_train_p = preprocess_image(X_train, is_training=True)\n",
        "# # X_val_p = preprocess_image(X_val, is_training=False)\n",
        "# # X_test_p = preprocess_image(X_test, is_training=False)\n",
        "\n",
        "# # One-hot Encoding for y\n",
        "# lab_enc = LabelEncoder()\n",
        "# y_train = lab_enc.fit_transform(y_train)\n",
        "# y_val = lab_enc.transform(y_val)\n",
        "# y_test = lab_enc.transform(y_test)\n",
        "# y_train = tf.keras.utils.to_categorical(y_train, num_classes=8)\n",
        "# y_val = tf.keras.utils.to_categorical(y_val, num_classes=8)\n",
        "# y_test = tf.keras.utils.to_categorical(y_test, num_classes=8)\n",
        "\n",
        "# # Visualising one hot encoding, showing first 5\n",
        "# for i in range(5):\n",
        "#     print(f\"Target after one-hot encoding in y_train[{i}]:\", y_train[i])\n",
        "\n",
        "# # Allowed us to view full processed section, ensure function worked as intended\n",
        "# # result = (X_train_processed[:1])\n",
        "# #np.set_printoptions(threshold=np.inf)\n",
        "# # print(result)\n",
        "\n",
        "# # Smote code BEGINS - Using SMOTE LOWERED ACCURACY (Mentioned in report eval for CNN) - DON'T USE\n",
        "# #sample_targ = {0: 2873, 1: 2873, 2: 2873, 3: 2873, 4: 2873, 5: 2873, 6: 2873, 7: 2873}\n",
        "# # smt = SMOTE(sampling_strategy='auto')\n",
        "# # X_train_smote, y_train_smote = smt.fit_resample(X_train_p.reshape(X_train_p.shape[0], -1), y_train)\n",
        "# # X_train = X_train_smote.reshape(-1, width, height, 3)\n",
        "\n",
        "# Smote code ENDS\n",
        "\n",
        "# # #4. preparing to build the network\n",
        "\n",
        "# Old portion of code, doesn't affect results\n",
        "# batch_size = 128\n",
        "# num_classes = 8\n",
        "# epochs = 32\n",
        "# save_dir = './'\n",
        "# model_name = 'keras_lfw_trained_model.h5'\n",
        "\n",
        "# # If using model, uncomment these lines - DO NOT USE - OLD CODE\n",
        "# # model = Sequential()\n",
        "# # model.add(Conv2D(32, kernel_size=(3, 3), strides=1, padding='same', input_shape= (128, 128, 3)))\n",
        "# # model.add(Activation('relu'))\n",
        "# # model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "# # model.add(Flatten())\n",
        "# # model.add(Dense(128))\n",
        "# # model.add(Activation('relu'))\n",
        "# # model.add(Dense(num_classes))\n",
        "# # model.add(Activation('softmax'))\n",
        "\n",
        "# # model.compile(loss='categorical_crossentropy',\n",
        "# #               optimizer='adam',\n",
        "# #               metrics=['accuracy'])\n",
        "\n",
        "# # model.summary()\n",
        "\n",
        "# # END OF MODEL\n",
        "\n",
        "# # # CODE FOR MODEL2 BEGINS\n",
        "# model2 = Sequential()\n",
        "# model2.add(Conv2D(32, kernel_size=(3,3), activation='relu', strides=1, kernel_regularizer=regularizers.l2(0.001), padding='same', input_shape=(width,height,3)))\n",
        "# model2.add(BatchNormalization())\n",
        "# model2.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "# model2.add(Conv2D(64, kernel_size=(3,3), activation='relu', kernel_regularizer=regularizers.l2(0.001), strides=1,padding='same'))\n",
        "# model2.add(BatchNormalization())\n",
        "# model2.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "# model2.add(Conv2D(128, kernel_size=(3,3), activation='relu', kernel_regularizer=regularizers.l2(0.001),strides = 1, padding = 'same'))\n",
        "# model2.add(BatchNormalization())\n",
        "# model2.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "# model2.add(Flatten())\n",
        "# model2.add(Dense(512))\n",
        "# model2.add(Activation('relu'))\n",
        "# model2.add(Dropout(0.35))\n",
        "# model2.add(Dense(num_classes))\n",
        "# model2.add(Activation('softmax'))\n",
        "\n",
        "# # #Potential Optimisers, SGD, Adam or RMSprop (Adam gave best result)\n",
        "# custom = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "# #custom = tf.keras.optimizers.SGD(learning_rate=0.0015,momentum=0.99)\n",
        "# # #custom = tf.keras.optimizers.RMSprop(learning_rate=0.001,rho=0.9)\n",
        "# # More code for Model2 compiling\n",
        "# model2.compile(loss='categorical_crossentropy',\n",
        "#               optimizer= custom,\n",
        "#               metrics=['accuracy'])\n",
        "# model2.summary()\n",
        "\n",
        "# history = model2.fit(X_train,y_train,verbose=1,epochs=30, batch_size=64,validation_data=(X_val_p,y_val))\n",
        "\n",
        "# # #5. make predictions\n",
        "\n",
        "# # If using model, uncomment this line\n",
        "# #pred = model.predict(X_test)\n",
        "# # If using model2, uncomment this line\n",
        "# pred = model2.predict(X_test_p)\n",
        "\n",
        "# pred = np.argmax(pred,axis=1)\n",
        "# y_compare = np.argmax(y_test,axis=1)\n",
        "# score = metrics.accuracy_score(y_compare, pred)\n",
        "# confus_mat = confusion_matrix(y_compare,pred)\n",
        "# print(\"Accuracy score: {}\".format(score))\n",
        "\n",
        "# # #6. plot data\n",
        "\n",
        "# #print(history.history.keys())\n",
        "# plt.figure(1)\n",
        "# plt.plot(history.history['loss'],color = 'blue')\n",
        "# plt.plot(history.history['val_loss'],color='red')\n",
        "# plt.title('Training/Validation loss')\n",
        "# plt.ylabel('Loss')\n",
        "# plt.xlabel('Epoch')\n",
        "# plt.legend(['Loss'],loc='upper left')\n",
        "# plt.show()\n",
        "\n",
        "# plt.figure(2)\n",
        "# plt.plot(history.history['accuracy'],color='blue')\n",
        "# plt.plot(history.history['val_accuracy'],color='red')\n",
        "# plt.title('Model Accuracy')\n",
        "# plt.ylabel('Accuracy')\n",
        "# plt.legend(['Accuracy'], loc='upper center')\n",
        "# plt.show()\n",
        "\n",
        "# #7. add confusion matrix to testing data\n",
        "# plt.figure(3,figsize=(8,8))\n",
        "# sns.heatmap(confus_mat, square=True, annot=True, cbar=False)\n",
        "# plt.xlabel('Predicted')\n",
        "# plt.ylabel('Actual')\n",
        "# plt.title('Confusion Matrix')\n",
        "# plt.show()\n",
        "\n",
        "\n",
        "# CODE FOR MODEL2 ENDS\n",
        "\n",
        "# # Code for MobileNetV2 BEGINS - We wanted to test how MobileNetV2 fared against our model\n",
        "\n",
        "# # Testing MobileNetV2:\n",
        "\n",
        "# # trained_model = tf.keras.models.Sequential([\n",
        "# #     MobileNetV2(weights='imagenet', input_tensor=tf.keras.layers.Input(shape=(width,height,3)),include_top=False),\n",
        "# #     tf.keras.layers.BatchNormalization(),\n",
        "# #     tf.keras.layers.Flatten(),\n",
        "# #     tf.keras.layers.Dense(512,activation='relu'),\n",
        "# #     tf.keras.layers.Dropout(0.3),\n",
        "# #     tf.keras.layers.Dense(8,activation='softmax')])\n",
        "\n",
        "# # trained_model.layers[0].trainable=False\n",
        "\n",
        "# # trained_model.summary()\n",
        "# # trained_model.compile(loss='categorical_crossentropy',\n",
        "# #                       optimizer=custom,\n",
        "# #                       metrics=['accuracy'])\n",
        "\n",
        "# # # Early Finishing to help end the epochs when validation accuracy doesn't improve/stagnates\n",
        "# # early_fin = EarlyStopping(monitor='val_loss',patience = 10,restore_best_weights=True)\n",
        "# # history2 = trained_model.fit(X_train,y_train,verbose=1,epochs=30,batch_size=128,validation_data=(X_val_p,y_val),callbacks=[early_fin])\n",
        "\n",
        "# # # Prediction for MobileNetV2\n",
        "# # pred = trained_model.predict(X_test)\n",
        "# # pred = np.argmax(pred,axis=1)\n",
        "# # y_compare = np.argmax(y_test,axis=1)\n",
        "# # score = metrics.accuracy_score(y_compare, pred)\n",
        "# # confus_mat = confusion_matrix(y_compare,pred)\n",
        "# # print(\"Accuracy score: {}\".format(score))\n",
        "\n",
        "# Loss graph\n",
        "# # plt.figure(1)\n",
        "# # plt.plot(history2.history['loss'],color = 'blue')\n",
        "# # plt.plot(history2.history['val_loss'],color='red')\n",
        "# # plt.title('Training/Validation loss')\n",
        "# # plt.ylabel('Loss')\n",
        "# # plt.xlabel('Epoch')\n",
        "# # plt.legend(['Train','Val'],loc='upper left')\n",
        "# # plt.show()\n",
        "\n",
        "#  Accuracy Graph\n",
        "# # plt.figure(2)\n",
        "# # plt.plot(history2.history['accuracy'],color='blue')\n",
        "# # plt.plot(history2.history['val_accuracy'],color='red')\n",
        "# # plt.title('Training/Validation Accuracy')\n",
        "# # plt.ylabel('Accuracy')\n",
        "# # plt.xlabel('Epoch')\n",
        "# # plt.legend(['Train','Val'],loc='upper left')\n",
        "# # plt.show()\n",
        "\n",
        "# Confusion matrix plot\n",
        "# # plt.figure(3,figsize=(8,8))\n",
        "# # sns.heatmap(confus_mat, square=True, annot=True, cbar=False)\n",
        "# # plt.xlabel('Predicted')\n",
        "# # plt.ylabel('Actual')\n",
        "# # plt.title('Confusion Matrix')\n",
        "# # plt.show()\n",
        "\n",
        "# # CODE FOR MOBILENETV2 ENDS\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}